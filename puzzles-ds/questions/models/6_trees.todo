===========================

Overview
------

Explain how it works in a few words:

  Decision trees works finding successively "cutting" points on the variables that would improve accuracy / information gain.

Whats the metric the model tries to optimize?:

  sum( loss(subtrees) )

  - Accuracy
  TP + TN / All
  Problematic with unbalanced data
  Prone to overfitting, doesnt generalize well to unseen data 

  - Information gain:
  ? Formula
  Bias towards features with high number of features
  Favor splits that generates more homogenuous groups

  - Gini (cross entropy)
  p(a)*p(1-a)

  Favor splits that generates more homogenuous groups


Algorithm / Fitting
-------

Which method do you use to fit your model?:




What's the complexity?:



Pseudo-code (with formulas):



How do you need to prepare the data for the model? (categorical? missing? outliers? scale? variance?):



Does the model have convergence problems? Does it have a random component or will the same training data always generate the same model? How do we deal with random effects in training?:



Explain how {core feature} is done:



How many parameters the model has to learn? is is feasible setup?:



Evaluation
-------

How you evaluate quality/performance of your model? Why not metric ______?:



How interpretable is the model?:



Does the model have any meta-parameters and thus require tuning? What are they? How would you choose the best one?:



Are the predictions calibrated? If not how should they be calibrated?:



Is it prone to over-fitting? How do you detect? What can be done about this?:




Model selection
-------

Does the model make any important assumptions about the data? When might these be unrealistic? How do we examine the data to test whether these assumptions are satisfied?:




What alternative models might we use for the same type of problem that this one attempts to solve, and how does it compare to those?:



Can it learn non-linearities? If so, how so? If not, is there any way around it?:


