
Data exploration
-------

How do you summarize the distribution of your data?:



How do you find/handle outliers or data points that skew data?:



What assumptions can you make? Why and when? (i.e When is it safe to assume "normal"):



How to efficiently represent 5 dimension in a chart or in a video?:

  3 axis + shape + color
  Project dimensions to or 2

How would you handle an imbalanced dataset?:

  Try to model and see if there is a problem
  Sufficient data?
  Negative downsample
  Fancy downsampling
  Different weights
  Rephrase the problem

How do you handle missing or corrupted data in a dataset?:

  - Completely at random: (missing doesnt correlate with other variables)
  Drop if its rare
  Input mean
  Bin it and treat as one category
  Use a dummy variable to 

  - Not at random:
  Input E[ m | X ] according to a model
  ? any other options


Dimensionality
------

What is the curse of dimensionality?:

  Uniform points: distance between them increases as dimension increases


What is the difference between density-sparse data and dimensionally-sparse data?:



What is the purpose of dimensionality reduction and why do we need it?:



What ways of reducing dimensionality do you know? Pros and cons?:



Data shape
-------

Explaing power transforms:


What is Box-Cox transformation? :


When to do transformations?:

  First, transform the Y variable to achieve homoscedasticity (constant variance). Then, transform the X variable to achieve linearity.

  to achieve linearity.
  to achieve homogeneity of variance, that is, constant variance about the regression equation.
  to achieve normality or, at least, symmetry about the regression equation.

  A transformation that achieves one of these goals often ends up achieving all three. This sometimes happens because when data have a multivariate normal distribution, the linearity of the regression and homogeneity follow automatically.

How would you turn unstructured data into structured data?:



How to check if a distribution is close to Normal? Why would you want to check it? What is a QQ Plot?:



Feature selection
-------

How do you select features? What are some good ways for performing feature selection that do not involve exhaustive search?:

  - Pre computed statistics
  * Mutual information
  Measure the reduction of uncertainty in Y after observing X. It is the KL distance between the joint density and the product of the individual densities. it measures how much knowing one of these variables reduces uncertainty about the other
  Cons: biased towards high cardinality features

  * Spearman:
  monolitically rank
  
  * Pearson:
  linear relationship

  * tests:
  f-test
  chi squared

  - Based on model (compared against validation set)

  * Stepwise selection

  
  * Lasso


  - Penalization of complexity

  * R^2 adj

  * AIC
  . -2*ln(likelihood) + 2*k,
  AIC tries to select the model that most adequately describes an unknown, high dimensional reality
  the results point to the fact that AIC is too liberal and still frequently prefers a more complex, wrong model over a simpler, true model

  * BIC
  . -2*ln(likelihood) + ln(N)*k
  BIC tries to find the TRUE model among the set of candidates


What to do with categorical variables of high cardinality?:

  1. Clustering on the behaviour you want to capture 
  2. Learning representations (auto-encoder, pca, matrix factorization, word2vec)
  3. Replace by informative number (conversion, demand, ...)
  4. Hashing trick
  5. Group rare ones
  6. Replace by frequency on the training set (non-linear methods)

  ? pros and cons

Can a feature that doesnt help alone, be useful when another feature is added?:

  Depends on the model and how they interact!
  For instance, a linear model wouldnt be able to learn anything from if x1 = sign(x)*x. 
  If we add another feature controlling for sign(x) and interact both, the model would be able to learn
  the non-linearity and control for discontinuity.

