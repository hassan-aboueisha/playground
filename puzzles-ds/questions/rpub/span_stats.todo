Statistics

1. How do you assess the statistical significance of an insight?:

    Randomized controlled trial
    - Randomly split your subjects
    - The size will be according to the power you want to have
    - Apply treatment on one group
    - Measure the effect
    - Do hypothesis testing

    Hypothesis testing
    - Given distribution of data (binomial? normal?)
    - Assuming there is no difference (null hypothesis)
    - Calculate probability of observing effect of this size or worse
    - If < X%, significant, otherwise is not

    Statistic significant doesnt mean its business relevant.

2. Explain what a long-tailed distribution is and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?:

    - In long tailed distributions, a high frequency population is followed by a low frequency population, which gradually tails off asymptotically
    
    - Rule of thumb: majority of occurrences (more than half, and when Pareto principles applies, 80%) are accounted for by the first 20% items in the distribution

    - Importance in classification and regression problems:
    Skewed distribution (training data biased, cant generalize on the long tail in most cased)
    Which metrics to use? Accuracy paradox (classification), F-score, AUC
    Issue when using models that make assumptions on the linearity (linear regression): need to apply a monotone transformation on the data (logarithm, square root, sigmoid function…)
    Issue when sampling: your data becomes even more unbalanced! Using of stratified sampling of random sampling, SMOTE (“Synthetic Minority Over-sampling Technique”, NV Chawla) or anomaly detection approach

3. What is the Central Limit Theorem? Explain it. Why is it important?:

    The CLT rules that averages of sufficiently large number of samples follow a normal distribution.
    
    It's important because its one of the building blocks to estimate values of a population
    from its samples, such as mean and deviation. Which is used in hypothesis testing and confidence interval building.


4. What is statistical power?:

    P( detecting effect | effect exists) ~ P( ~H0 | H1 )
        ~  effect / sterr
        ~ effect * sq(n) / std

    The bigger the effect, the bigger the power.
    The bigger the sample size, the bigger (root) the power.
    The bigger the deviation, the lower the power.
    (Hidden: the bigger the threshold, the bigger the power)

    
5. Explain selection bias (with regard to a dataset, not variable selection). Why is it important? How can data management procedures such as missing data handling make it worse?:

    Selection bias happens when the sample is not random (there was something wrong on the process of selecting the subjects)
    
    For instance >
    - Poll on an election sampled only from a biased source (golf clubs)
    - Dataset to train fraud detection only on people that were not classified as so

    Why is important?
    - It introduces hidden bias into the model
    - Affects the ability of the model to generalize on the true population  

    How munging can make it worse?
    - When hidden values means something (empty address on forms -> privacy aware)
    - When population that interact with effect is more/less likely to be considered

6. Provide a simple example of how an experimental design can help answer a question about behavior. How does experimental data contrast with observational data?:

    Is extremely difficult to derive causality from observational data, as you dont 
    know the confounding factors behind an observed effect.

    For example >
    - You observe that people with better connection search less.
    - Would be the relation "I have better connection therefore i'll search less" or "I searched less therefore i was classified as better connection"?
    - Doing an randomized trial we could make a step further proving/disproving causality:
    - Split users in random 2 groups. Serve slower pages to one group.
    - Hypothesis: people with slower connections search less.


7. Is mean imputation of missing data acceptable practice? Why or why not?:

    Yes, depends on your application.

    - Things to consider >
    caution to not leak information from test set
    will change deviation of population

    - Some common practices for handling missing data > 
    Discard if is measurement error and rare
    Categorical: treat as one category
    Real: simple estimate (average or regression)
    Real: add another dummy variable that activates on missing
    Real: bin it and treat as categorical


8. What is an outlier? Explain how you might screen for outliers and what would you do if you found them in your dataset. Also, explain what an inlier is and how you might screen for them and what would you do if you found them in your dataset:

    ?

9. How do you handle missing data? What imputation techniques do you recommend?:

    - Completely at random: (missing doesnt correlate with other variables)
    Drop if its rare
    Input mean
    Bin it and treat as one category
    Use a dummy variable to 

    - Not at random:
    Input E[ m | X ] according to a model
    ? any other options


10. You have data on the durations of calls to a call center. Generate a plan for how you would code and analyze these data. Explain a plausible scenario for what the distribution of these durations might look like. How could you test, even graphically, whether your expectations are borne out?:


    **

11. Explain likely differences between administrative datasets and datasets gathered from experimental studies. What are likely problems encountered with administrative data? How do experimental methods help alleviate these problems? What problem do they bring?:

    Administrative datasets=observational
    Experimental studies=randomized trial (hopefully)

    **


12. You are compiling a report for user content uploaded every month and notice a spike in uploads in October. In particular, a spike in picture uploads. What might you think is the cause of this, and how would you test it?:

    **

13. You’re about to get on a plane to Seattle. You want to know if you should bring an umbrella. You call 3 random friends of yours who live there and ask each independently if it’s raining. Each of your friends has a 2/3 chance of telling you the truth and a 1/3 chance of messing with you by lying. All 3 friends tell you that “Yes” it is raining. What is the probability that it’s actually raining in Seattle?:

    P(rain) = x
    P(rain | friend) = x'

14. There’s one box - has 12 black and 12 red cards, 2nd box has 24 black and 24 red; if you want to draw 2 cards at random from one of the 2 boxes, which box has the higher probability of getting the same color? Can you tell intuitively why the 2nd box has a higher probability:

    12b12r 24b24r

    first card: doesnt matter
    second card: 1st= 11/23, 2nd= 23/47 

    Second box because the proportion of red balls to the total is bigger, so more chances of picking the right ball.

15. What is: lift, KPI, robustness, model fitting, design of experiments, 80/20 rule?:

    - List:

    - KPI:

    - Model fitting:

    - Design of experiments:

    - 80/20 rule:


17. Give examples of data that does not have a Gaussian distribution, nor log-normal.:

    - Exponential family:
    Poisson: Common for counts, adding or multiplying poissons still follows poisson
    Chi-squared: Arises as sum of squared normal variables (so used for variances)
    Gamma: Right skewed and useful for things with a natural minimum at 0. the time between k events on a poisson process
    Beta: Defined between 0 and 1, useful for proportions or other quantities that must be between 0 and 1
    Dirichlet: ?
    Geometric: number of failures before 1st success (special case of negative binomial)
    Bernoulli: number of successes with a probability in a trial (special case binomial)
    Categorical: k possible outcomes with a given probability in a trial (special case multinomial)

    - Relevant:
    Binomial: how many "successes" out of a given number of independent trials with same probability of "success"
    Multinomial: ?
    Negative binomial: counts with mininum 0 (number of failures before k success)
    Exponential: the time between events on a poisson process
    Student's t: ?
    Uniform: ? 
    Hypergeometric: ?

    - Conjugates:
    Beta -> binomial   (single probability)
    Gamma -> poisson   (non-negative numbers)
    Dirichlet -> multinomial  (vector of probabilities)

    - Generalization:
    Binomial -> bernouli
    Multinomial -> categorical
    Dirichlet -> beta
    Gamma -> exponential


18. What is root cause analysis? How to identify a cause vs. a correlation? Give examples:

    - The only true way of doing it:
    Randomized controlled trials

    - Somethings can give you hints:
    Many studies showing strong correlation
    ?


19. Give an example where the median is a better measure than the mean:

    Both: measures central tendency.

    - When is bad:
    Exponential distribution (book window)
    Heavy tail skewed distributions (salary)
    Bimodal distributions (frequency of gym)

    - When is good:
    Median is sensitive to sample size (small samples it will jump a ot)
    Linear optimization (mean is the center of OLS )
    Normal data (more efficient )
    Resistant to shift changes


20. Given two fair dices, what is the probability of getting scores that sum to 4? to 8?:

    P(X + Y = 4) = 3/36 => (2,2) (3,1)*2
    P(X + Y = 8) = 5/36 => (4,4) (3,5)*2 (2,6)*2 => 5/36

    MDF =>
    2 33 444 5555 66666 777777 88888 9999 101010 1111 12

21. What is the Law of Large Numbers?:

    Given a large number of trials, the frequency of values of a random variable will converge to
    its probability. 

22. How do you calculate needed sample size?:

    - By power analysis

    power ~ abs effect size / sterr ~ abs effect size * n / deviation
    Sample size will be proportional to wanted power and deviation (which is a proxy dataset noise) and inversely proportional to effect size. 

    ? any other ways


23. When you sample, what bias are you inflicting?:

    Bias is a systematic error introduced in the analysis or dataset.

    - Type of biases:
    Sampling bias: subjects being more or less likely to be included than others
    Selection bias: randomization was not achieved (not representative of population) (same as sampling bias)
    Response bias: some subjects are more likely to answer than others
    Detection bias: effect is more likely to be detected in a subset of subjects
    Reporting bias: some pbservations are more likely to be reported than others
    Observer bias: the experimenter influences the experiment
    Survivor bias: analysis did after a certain selection process (which a group is more or less likely to pass)
    Estimator: ? 

    - Hopefully none.
    Selection bias (not random)
    Survivor (picking only the ones that passed a stage)
    Under coverage (too small for the targetted measurement)

24. How do you control for biases?:

    - How do you control pre modeling time?
    Choose a representative sample, preferably by a random method
    Choose an adequate size of sample
    Identify all confounding factors/biases if possible (rank of item for example)

    "If entire segments of the population are excluded from a sample, then there are no adjustments that can produce estimates that are representative of the entire population. But if some groups are underrepresented and the degree of underrepresentation can be quantified, then sample weights can correct the bias"

    - How do you control modeling time?
    Added confounding factors as independent variables
    Prefer models with lower bias / higher variance
    Prevent overfitting/underfitting
    Properly evaluate performance (resampling methods)

    - How do you control post modeling time?
    Monitoring measurements (response distribution, ground truth distribution)
    Monitoring population feature space (how they differ from modelling? changing over time?)


25. What are confounding variables?:

    Confounding variables are variables that might explain the measured effect.
    ?


26. What is A/B testing?:

    Online randomized controlled trials at scale.

    REPEATED

27. An HIV test has a sensitivity of 99.7% and a specificity of 98.5%. A subject from a population of prevalence 0.1% receives a positive test result. What is the precision of the test (i.e the probability he is HIV positive)?:

    P = 0.1%
    TPR recall = 99.7
    TNR = 98.5%
    P( sick | + ) = P( sick, +) =  P(+|sick )*P(sick)
                     P (+)         P(+|sick)*P(sick) + P(+|~sick)*P(~sick)

28. Infection rates at a hospital above a 1 infection per 100 person days at risk are considered high. An hospital had 10 infections over the last 1787 person days at risk. Give the p-value of the correct one-sided test of whether the hospital is below the standard:

    high > 1/100
    Probability of observing something below than standard
    CDF_poisson(k=10, u=1780*0.01) = 0.03


29. You roll a biased coin (p(head)=0.8) five times. What’s the probability of getting three or more heads?:

    CDF_binom( 2, 5, 0.8 ) 

30: A random variable X is normal with mean 1020 and standard deviation 50. Calculate P(X>1200):

    X = N(1020, 50)
    P(X>1200) = 1 - CDF(1200, 1020, 50) = 0.0002
    In terms of sigma: 1200-1020= 180 / 50 => 3.6 sigma
    1 -> 64%
    2 -> 95%
    3 -> 99%

31: Consider the number of people that show up at a bus station is Poisson with mean 2.5/h. What is the probability that at most three people show up in a four hour period?:

    X = Poisson(10)
    P( X < 3 ) = CDF(k=3, u=10) = 1.03%

32: you are running for office and your pollster polled hundred people. 56 of them claimed they will vote for you. Can you relax?:

    H0=0.5 
    Ha=0.56

    - By interval
    H0 = 50 +- 10
    H1 = 56 +- 9.92
    
    - By hypothesis
    CDF -> 10%

33: Geiger counter records 100 radioactive decays in 5 minutes. Find an approximate 95% interval for the number of decays per hour.:

    ?


34: The homicide rate in Scotland fell last year to 99 from 115 the year before. Is this reported change really networthy?:

    h0 = 115
    hA = 99

    p_Poisson = CDF(k=99, u=115) ~7%

35: Consider influenza epidemics for two parent heterosexual families. Suppose that the probability is 17% that at least one of the parents has contracted the disease. The probability that the father has contracted influenza is 12% while the probability that both the mother and father have contracted the disease is 6%. What is the probability that the mother has contracted influenza?:

    P( f or m ) = 17%
    P( f      ) = 12%
    P( m&f    ) = 6%

    P( f or m) = P(m) + P(f) - P(m&f) 
    17% = x + 12% - 6% -> 11%


36: Suppose that diastolic blood pressures (DBPs) for men aged 35-44 are normally distributed with a mean of 80 (mm Hg) and a standard deviation of 10. About what is the probability that a random 35-44 year old has a DBP less than 70?:

    d = N(u=80, std=10)
    1 std-dev > 68% => 32% is beyond, 16% on the first  

37: In a population of interest, a sample of 9 men yielded a sample average brain volume of 1,100cc and a standard deviation of 30cc. What is a 95% Student’s T confidence interval for the mean brain volume in this new population?:

    stderr = 30/3 = 10
    1100 +- 10*multiplier


38: A diet pill is given to 9 subjects over six weeks. The average difference in weight (follow up - baseline) is -2 pounds. What would the standard deviation of the difference in weight have to be for the upper endpoint of the 95% T confidence interval to touch 0?:

    -2 / sq(9) = -0.667
    multiplier = 2 / x



39. In a study of emergency room waiting times, investigators consider a new and the standard triage systems. To test the systems, administrators selected 20 nights and randomly assigned the new triage system to be used on 10 nights and the standard system on the remaining 10 nights. They calculated the nightly median waiting time (MWT) to see a physician. The average MWT for the new system was 3 hours with a variance of 0.60 while the average MWT for the old system was 5 hours with a variance of 0.68. Consider the 95% confidence interval estimate for the differences of the mean MWT associated with the new system. Assume a constant variance. What is the interval? Subtract in this order (New System - Old System).:

    ?


40. To further test the hospital triage system, administrators selected 200 nights and randomly assigned a new triage system to be used on 100 nights and a standard system on the remaining 100 nights. They calculated the nightly median waiting time (MWT) to see a physician. The average MWT for the new system was 4 hours with a standard deviation of 0.5 hours while the average MWT for the old system was 6 hours with a standard deviation of 2 hours. Consider the hypothesis of a decrease in the mean MWT associated with the new treatment. What does the 95% independent group confidence interval with unequal variances suggest vis a vis this hypothesis? (Because there’s so many observations per group, just use the Z quantile instead of the T.):

    ?