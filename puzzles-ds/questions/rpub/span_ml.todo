

1. What is cross-validation? How to do it right?:


    Cross-validation is a method to estimate metrics on unseen data. We partition the dataset into k partitions, and use them to train/test the model k times, rotating which partition is used for testing. 

    To do it right, many things to consider >
    - sampling method (stratified x non-stratified)
    - number of partitions. Compromise between bias x variance x resources.
    Low k => high bias, low variance, low resource; High k => low bias (lower bound), high variance, low;

    ? bias variance of sampling methods


2. Is it better to design robust or accurate algorithms?:


    Depends on the business case. 
    If we consider 
    - Robust   => lower variance
    - Accurate => lower bias

    If we're predicting patients with cancer, we would prefer accuracy (cost attached to a false positives/negative are too high).

    ? Examples where variance is better than bias


3. How to define/select metrics?:

    - Things to consider:
    Problem domain
    Dataset characteristics (imbalance, data type
    Goal (what we're trying to opmitize)

    - Regression:
    MSE
    R2
    RMSE

    - Classification:
    Accuracy
    Precision
    Recall
    Logloss
    AUC

    - Multiclass:
    asd

    - Ranking:
    Precision@k
    NDSG
    MAV

    - Clustering:
    AIC
    BIC
    Intra cluster variance


4. Explain what regularization is and why it is useful. What are the benefits and drawbacks of specific methods, such as ridge regression and lasso?:


    - Regularization is a way to constrain the learning to avoid overfitting (better generalization). It can be done in many ways >
    penalizing big coefficients
    forcing non useful features to zero
    randomly deactivating nodes

    - Comparing the methods >
    
    L1 (lasso) min( loss(X,Y,B) + t|B| ). Introduce sparcity (remove features entirely instead of penalizing big coefficients), unstable, good for feature selection.

    L2 (ridge) min( loss(X,Y,B) + t|B2| ). 

    - ? benefits and drawbacks 



5. Explain what a local optimum is and why it is important in a specific context, such as K-means clustering. What are specific ways of determining if you have a local optimum problem? What can be done to avoid local optima?:

    Local optimum >
    the algorithm converged to a solution better than neighbors but not necessarily the best.

    How to detect >
    - algorithm had strong bindings to it initialization state
    - non-convex problem

    How to avoid >
    - repeat k times randomizing initialization
    
    ? avoiding local minina
    ? detect non-convex problems


6. Assume you need to generate a predictive model using multiple regression. Explain how you intend to validate this model:

    For linear regression >
    - RMSE on unseen data (square[(y- y)2]/n)
    - Residual analysis (residual should be normal)
    - 

    ? residual analysis / Heteroskedasticity
    ? metrics linear regression


    For logistic regression >
    - lr x baseline (most frequent class)
    - precision recall curve (threshold analysis)
    - response chart 
    - calibration curve


7. Explain what precision and recall are. How do they relate to the ROC curve?:

    Precision = % positive predictions predicted true
    Recall = % positive instances predicted true

    One model will be dominant on ROC space if and only if its dominant on ROC curve (FPR, TPR).

    Benefits >
    - roc curve you can compare different models easily (pr curve will diverge if base rate is different)
    - pr curve will give you more insights on imbalanced datasets (auc roc is insensitive if you move the needle from fp to fn)
    - roc curve gives you an average metric to optimize for, if you dont care about the cost of FP / FN (if you care, either optimize for range of auc or trying to maximizing precision-recall)

    ? why roc curve is not good

8. What is latent semantic indexing? What is it used for? What are the specific limitations of the method?:
    
    ?


9. Explain what resampling methods are and why they are useful:

    Resampling methods are tools to help us give better estimates on unseen data.

    Bootstrap: take repeated samples with replacement. If you do it many times, it will average the quantity you're estimating.

    Crossvalidation >

    When to use it?
    - Test data is scarce
    - Evaluate stability / uncertainty
    - Introduce stochastic components (rf)

    Cons of using it >
    - 

    ? when to use bootstrap

10. What is principal component analysis? Explain the sort of problems you would use PCA for. Also explain its limitations as a method:

    PCA is a dimensionality reduction technique where we try to explain the most of the information on the original data with K latent vectors (uncorrelated variables).

    Algorithm >
    1) Preprocessing (standardization): PCA is sensitive to the relative scaling of the original variable
    2) Compute covariance matrix ΣΣ
    3) Compute eigenvectors of ΣΣ
    4) Choose kk principal components so as to retain xx% of the variance (typically x=99x=99)

    Applications >
    1) Compression
    - Reduce disk/memory needed to store data
    - Speed up learning algorithm. Warning: mapping should be defined only on training set and then applied to test set

    Visualization: 2 or 3 principal components, so as to summarize data

    Limitations >
    - PCA is not scale invariant
    - The directions with largest variance are assumed to be of most interest
    - Only considers orthogonal transformations (rotations) of the original variables
    - PCA is only based on the mean vector and covariance matrix. Some distributions (multivariate normal) are characterized by this but some are not
    - If the variables are correlated, PCA can achieve dimension reduction. If not, PCA just orders them according to their variances


11. Explain what a false positive and a false negative are. Why is it important these from each other? Provide examples when false positives are more important than false negatives, false negatives are more important than false positives and when these two types of errors are equally important:

    FP: positive result when there is no effect
    FN: negative result when there was a effect

    When false positives are more important than false negatives >
    - In a non-contagious disease, where treatment delay doesn’t have any long-term consequences but the treatment itself is grueling
    - HIV test: psychological impact

    When false negatives are more important than false positives >
    - If early treatment is important for good outcomes
    - In quality control: a defective item passes through the cracks!
    - Software testing: a test to catch a virus has failed

12. What is the difference between supervised learning and unsupervised learning? Give concrete examples:

    Supervised: you have a goal of predicting something that you have on the data (labeled data).
    Unsupervised: discover patterns.

    Supervised >
    - Predict prices of stock
    - Model the probability of disease

    Unsupervised >
    - Find group of similar users
    - Find the topic of the text


13. What does NLP stand for?:

    NLP stands for Neuro-language-processing.

    Some techniques that can be used >
    - Classification (sentiment analysis, tpoic detection labeled data)
    - K-means
    - NMF. 
    - LDA. Unsupervised - 

    Tools that help represent the text >
    - Count of words:

    - TF-IDF

    Term frequency inverse document frequency. Weight the frequency of the term on the document against how frequent it is on the corpus, in the sense that the bigger the frequency in the text and the less popular on the corpus the more signal we have. Weighted against document length, so we dont bias towards big documents.  
    
    TDIDF = Count(term|doc)   * log( size(corpus)    )
                   size(doc)             count(doc|term)
    Cons: vector of the size of the vocabulary.

    - Stemming:

    - Tokenization:

    - Stop words:

    - word2vec

    - n grams


14. What are feature vectors?:

    Feature vectors are numerical representations of an object.

    What to do to clean your dataset?
    - Missing values
    - Outliers
    - Correlated variables

    How to select features?

15. When would you use random forests Vs SVM and why?:

    When to use
    - Multiclass
    - Nonlinear boundaries
    - Small amount of time (good defaults)
    - Lots of missing info

    Random forests
    - Ensemble method based on bootstraps
    - Take N bootstrapped samples, for each node take K random features, evaluate information gain for the splits in all of them (which splits to consider?), make the split and proceed. Prediction is average of the N trees (regression, average of leafs, classification majority vote or frequency of class).
    - Optimizes gini impurity (how uniform is the tree / how often a random node would be misclassified)
        sum(p(class_i) * p(~class_i))
    - Information gain (entropy)
        -sum( p(class_i)* log(p(class_i)) ) )  
    - Can learn non-linearities, sensitive to variations on the data, handle missing values, good defaults.

    ? Trees algorithm
    ? svm internals

16. How do you take millions of users with 100’s transactions each, amongst 10k’s of products and group the users together in meaningful segments?:

    Alternative 1
    - U x P: 1 when bought, 0 when didnt by
    - Kmeans with cosine similarity (distance would suffer from dimensionality and not real)
    - Drawbacks: scalability, local optima

    Alternative 2
    - kNN

    Alternative 3
    - Projection in a lower dimension
    - matrix factorization -> highly scalable


    +++++++++++++++++++++++++



17. How do you know if one algorithm is better than other?:

    - Pre-analysis
    1. Application (random forest x neural nets for image recognition x fraud detection)
    2. Scalability (what is the size of the problem?)
    3. Out of core learning (can the algorithm keep on learning?)
    4. Dataset (missing values? inbalanced? linear relationships?) 
    5. Bias x variance (sensitivity to changes on the underlying distribution)
    
    - Given a problem and a dataset
    1. Crossvalidation with a common metric
    2. Analysis of prediction distribution (most metrics are averages)
    3. Prefer the one with smaller complexity (occam razor)
    4. Bootstrap and count number of times one is bette 
    5. Live AB testing

18. How do you test whether a new credit risk scoring model works?:

    - What i'd expect: the higher the score, the higher the default rate
    Problem: we have data only on people that previous algorithm didnt consider superbad
    What we're trying to maximize: profit.
        E(profit|model) = -p(default|model)*credit + p(~default|model)*profit. Where P(default) = FNR and P(~default) = TNR
    Decision: While discovering more true positives is good, the cost of a false negative (not detecting a bad application) is higher than the cost of a false positive (rejecting a good application), so we might tend towards recall.

    - Offline analysis >
    1. Run in a dataset not used for training and get offline metric (cross entropy)
    2. Calibration curve ( probability x % of predicted )
    3. PR curve (decide the threshold)

    - Online analysis: (as our data have selection bias, we cant measure the true impact)
    1. For a period: AB test on new customers accepting only customer < threshold
    2. Track default rate (FNR), credit lost (the distribution might be different) and profit 
    3. Hypothesis testing: whats the proassuming there is no difference.
    4. Whats the cycle time?

19. What is: collaborative filtering, n-grams, cosine distance?:

    - Collaborative filtering
    Its an approach to learning recommendations, based on behaviour of similar users / items.
    Its composed of 2 mainstream areas: user based, item based, content based.

    * user based:

    * item based:

    * loss:


    - N-grams:

    - Cosine-distance:

20. What is better: good data or good models? And how do you define “good”? Is there a universal good model? Are there any models that are definitely not so good?:

    - Good data. Which extreme you would prefer?
    Handful of features based on domain experts + a linear model.
    Random features + highly complex model.

    - Also, data defines the upper bound a model can learn (Bayes error). So there is only much one can do within those boundaries -> if you increase data quality, you change Bayes error and maybe step out of local minima.

    - Bayes model is the perfect model, but untratable. What models can do is try to approximate it. So different problems might require different approachs.
    Is your data categorical or numerical?
    Whats the imbalance?
    How much is it spread?
    Labels are defined?
    Missing inputs? 
    How they relate with predictor?


21. Why is naive Bayes so bad? How would you improve a spam detection algorithm that uses naive Bayes?:

    - Strong assumption: feature independence given labels.
    P ( y | x) = P ( x | y) = PR( P (x_i|y) ) * P(y)
                 P ( x )      P(x)
    - Pseudocode:
    Count( label )
    Categorical: Count( feature_value & label) / Count( label )
    Numerical: ll( value, mean(feature|label), std(feature|label) ) -> assumption of distribution

    - Laplace smoothing: (when rare features n -> 0 )
    n + k
    N + k*d(x_i | label)

    - Is it using TF-IDF? Count vectorizer?
    Bin features ( denser data )
    Extract independent vectors

22. What are the drawbacks of linear model? Are you familiar with alternatives (Lasso, ridge regression, boosted trees)?:

    - Strong Assumptions:
    ?

    - Benefits
    Resistent to changes on the data
    Simple to compute
    Can be solved with out of core methods

    - Drawbacks
    Only linear relationships (although can be tweaked with interactions and base expansions)
    ?


23. Do you think 50 small decision trees are better than a large one? Why?:

    Depends on the problem and how do you fit them.
    Do you need to lower the bias or the variance?
    How the trees are different from each other?

    - Larger tree
    Sparcity on the nodes -> less balanced predictions
    Tend to overfit -> it will adapt to training data (possible learning the noise?)
    Small changes on dataset change the model

    - Multiple different trees
    More robust (less variable) solution as its averaging between multiple models
    Smaller change to capture the noise, as the trees are smaller and prediction is made by majority
    More expensive to compute and less interpretable


24. Why is mean square error a bad measure of model performance? What would you suggest instead?:

    ?

25.How can you prove that one improvement you’ve brought to an algorithm is really an improvement over not doing anything? Are you familiar with A/B testing?:

    - Offline:
    Classifiers: improvement over base rate (predicting the predominat class)
    Regression: improvement over predicting the mean
    Ranking: improvement over most popular



26. What do you think about the idea of injecting noise in your data set to test the sensitivity of your models?:


27. Do you know / used data reduction techniques other than PCA? What do you think of step-wise regression? What kind of step-wise techniques are you familiar with?:


28. How would you define and measure the predictive power of a metric?:


29. Do we always need the intercept term in a regression model?:


30. What are the assumptions required for linear regression? What if some of these assumptions are violated?:


31. What is collinearity and what to do with it? How to remove multicollinearity?:


32. How to check if the regression model fits the data well?:


33. What is a decision tree?:


34. What impurity measures do you know?:


35. What is random forest? Why is it good?:


36. How do we train a logistic regression model? How do we interpret its coefficients?


37. What is the maximal margin classifier? How this margin can be achieved and why is it beneficial? How do we train SVM?


38. What is a kernel? Explain the kernel trick


39. Which kernels do you know? How to choose a kernel?


40. Is it beneficial to perform dimensionality reduction before fitting an SVM? Why or why not?

41. (What is an Artificial Neural Network?) What is back propagation?


42. What is curse of dimensionality? How does it affect distance and similarity measures?

43. What is Ax=b? How to solve it?

44. How do we multiply matrices?

45. What is singular value decomposition? What is an eigenvalue? And what is an eigenvector?

46. What’s the relationship between PCA and SVD?

47. Can you derive the ordinary least square regression formula?

48. What is the difference between a convex function and non-convex?

49. What is gradient descent method? Will gradient descent methods always converge to the same point?

50. What the Newton’s method is?

51. Imagine you have N pieces of rope in a bucket. You reach in and grab one end-piece, then reach in and grab another end-piece, and tie those two together. What is the expected value of the number of loops in the bucket?