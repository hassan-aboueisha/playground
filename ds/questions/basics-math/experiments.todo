
Experimental design
-------

How do you create test and control groups?:



How do you control for external factors?:



How do you evaluate results?:



Why should the test and the control group be of the same size in an "A/B" test?:



Why is randomization important in experimental design?:



How can you prove that one improvement youâ€™ve brought to an algorithm is really an improvement over not doing anything? How familiar are you with A/B testing?:


How do you decide which test to use?:


How to identify a cause Vs a correlation? Give examples.:


Sampling
------

What is statistical power?:

  P( detecting effect | effect exists) ~ P( ~H0 | H1 )
      ~  effect / stderr
      ~ effect * sq(n) / std

  The bigger the effect, the bigger the power.
  The bigger the sample size, the bigger (root) the power.
  The bigger the deviation, the lower the power.
  (Hidden: the bigger the threshold, the bigger the power)


Explain selection bias (with regard to a dataset, not variable selection). Why is it important? How can data management procedures such as missing data handling make it worse?:

  Selection bias happens when the sample is not random (there was something wrong on the process of selecting the subjects)
  
  For instance >
  - Poll on an election sampled only from a biased source (golf clubs)
  - Dataset to train fraud detection only on people that were not classified as so

  Why is important?
  - It introduces hidden bias into the model
  - Affects the ability of the model to generalize on the true population  

  How munging can make it worse?
  - When hidden values means something (empty address on forms -> privacy aware)
  - When population that interact with effect is more/less likely to be considered


How do you calculate needed sample size?:

  - By power analysis

  power ~ abs effect size / sterr
        ~ abs effect size * n / deviation(sample)
  Sample size will be proportional to wanted power and deviation (which is a proxy dataset noise) and inversely proportional to effect size. 

  ? any other ways


How do you control for biases?:

  - How do you control pre modeling time?
  Choose a representative sample, preferably by a random method
  Choose an adequate size of sample
  Identify all confounding factors/biases if possible (rank of item for example)

  "If entire segments of the population are excluded from a sample, then there are no adjustments that can produce estimates that are representative of the entire population. But if some groups are underrepresented and the degree of underrepresentation can be quantified, then sample weights can correct the bias"

  - How do you control on modeling time?
  Added confounding factors as independent variables
  Prefer models with lower bias / higher variance
  Prevent overfitting/underfitting
  Properly evaluate performance (resampling methods)

  - How do you control post modeling time?
  Monitoring measurements (response distribution, ground truth distribution)
  Monitoring population feature space (how they differ from modelling? changing over time?)


When you sample, what bias are you inflicting?:

  Bias is a systematic error introduced in the analysis or dataset.

  - Type of biases:
  Sampling bias: subjects being more or less likely to be included than others
  Selection bias: randomization was not achieved (not representative of population) (same as sampling bias)
  Response bias: some subjects are more likely to answer than others
  Detection bias: effect is more likely to be detected in a subset of subjects
  Reporting bias: some pbservations are more likely to be reported than others
  Observer bias: the experimenter influences the experiment
  Survivor bias: analysis did after a certain selection process (which a group is more or less likely to pass)
  Estimator: ? 

  - Hopefully none.
  Selection bias (not random)
  Survivor (picking only the ones that passed a stage)
  Under coverage (too small for the targetted measurement)


Some 3rd party organization randomly assigned people to control and experiment groups. How can you verify that the assignment truly was random?:


Methods for sampling? Why they are useful? What are their limitations?:

  - Types (simple x clustered x stratified x ...)
  ?

  - Mode (with x without replacement)
  ?


Random
------

How do you assess the statistical significance of an insight?:

  Randomized controlled trial
  - Randomly split your subjects
  - The size will be according to the power you want to have
  - Apply treatment on one group
  - Measure the effect
  - Do hypothesis testing

  Hypothesis testing
  - Which metric? (success? average? count?)
  - Given distribution of data (binomial? normal?)
  - Assuming there is no difference (null hypothesis)
  - Calculate probability of observing effect of this size (on H0 sample parameters) or worse
  - If < X%, significant, otherwise is not

  Statistic significant doesnt mean its business relevant.


Provide a simple example of how an experimental design can help answer a question about behavior. How does experimental data contrast with observational data?:

  Is extremely difficult to derive causality from observational data, as you dont 
  know the confounding factors behind an observed effect.

  For example >
  - You observe that people with better connection search less.
  - Would be the relation "I have better connection therefore i'll search less" or "I searched less therefore i was classified as better connection"?
  - Doing an randomized trial we could make a step further proving/disproving causality:
  - Split users in random 2 groups. Serve slower pages to one group.
  - Hypothesis: people with slower connections search less.


Explain likely differences between administrative datasets and datasets gathered from experimental studies. What are likely problems encountered with administrative data? How do experimental methods help alleviate these problems? What problem do they bring?:

  Administrative datasets=observational
  Experimental studies=randomized trial (hopefully)

  **

What are confounding variables?:

  Confounding variables are variables that might explain the measured effect.
  ?

