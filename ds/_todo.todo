
Stats:
 ✔ Sampling methods @done (17-06-04 23:38)
 ✔ Purpose of common distributions @done (17-06-04 13:07)
 ✔ Types of biases @done (17-06-04 13:07)
 ✔ Residual analysis (when to transform X x Y, Heteroscedasticity) @done (17-06-11 09:56)
 ☐ Anova
 ☐ Distribution operations (sum, confidence interval building ...)
 ☐ Common tests( chi-squared, t-test, sign test, Kolmogorov-Smirnov )
 ☐ Zipf’s law, Pareto distribution, power laws
 ☐ Mixtures
 ☐ Observational studies / propensity score matching

ML:
 ✔ Curse of dimensionality (layman explanation) @done (17-06-04 13:07)
 ✔ Accuracy paradox @done (17-06-04 23:40)
 ✔ Model based clustering @done (17-06-04 23:40)
 ☐ Distance metrics( Hamming distance, Mahalanobis, ... ) 
 ☐ Kernel trick
 ✔ BIC, AIC, Mutual information @done (17-06-11 09:56)
 ☐ Bayes error
 ☐ Calibration methods
 ✔ Likelihood @done (17-06-11 09:56)

Math:
 ☐ Proability operations
 ☐ Detecting convexity
 ☐ Non-differentiable functions
 ☐ Basic algebra (cross product)

Calendar:
  - Toy problem + chapter + questions
  ☐ Mon: Logistic
  ☐ Tue: Kmeans
  ☐ Wed: Dimensionality
  ☐ Thu: Trees
  ☐ Fri: Recs + Nlp
  ☐ Sat: Others
  ☐ Sun: Modelling + Fitting
  ☐ Mon: review
  ☐ Tue: review
  ☐ Wed: review
  ☐ _

Doubts:
  ☐ How in depth should know alternative methods? (agglomerative clustering, ...)
  ☐ How in depth should know optimizations? (tree split, gradient descent variations, ...)
  ☐ How in depth should know NLP? (lda)
  ☐ How in depth should know statisticals tests? ()
  ☐ Dimensionality reduction - PCA is enough or should i know in-depth auto encoders, matrix factorization, ...



