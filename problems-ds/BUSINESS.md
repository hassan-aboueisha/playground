


## Business

What is the biggest data set that you have processed and how did you process it? What was the result?



Tell me two success stories about your analytic or computer science projects? How was the lift (or success) measured?




You are about to send one million email (marketing campaign). How do you optimize delivery and its response? Can both of these be done separately?



How would you turn unstructured data into structured data?



Can you perform logistic regression with Excel? If yes, how can it be done? Would the result be good?



Give examples of data that does not have a Gaussian distribution, or log-normal. Also give examples of data that has a very chaotic distribution?



How can you prove that one improvement youâ€™ve brought to an algorithm is really an improvement over not doing anything?



What is sensitivity analysis? Is it better to have low sensitivity and low predictive power?



How do you perform good cross-validation?



What do you think about the idea of injecting noise in your data set to test the sensitivity of your models?



Compare logistic regression with decision trees and neural networks. How have these technologies improved over the last 15 years?



What is root cause analysis?



How to identify a cause Vs a correlation? Give examples.



How to efficiently represent 5 dimension in a chart or in a video?



Which is better: Too many false positives or too many false negatives?



Have you used any of the following: Time series models, Cross-correlations with time lags, Correlograms, Spectral analysis, Signal processing and filtering techniques? If yes, in which context?



What is the computational complexity of a good and fast clustering algorithm?



What is a good clustering algorithm?



How do you determine the number of clusters? How would you perform clustering in one million unique keywords, assuming you have 10 million data points and each one consists of two keywords and a metric measuring how similar these two keywords are? How would you create this 10 million data points table in the first place?




Certain third party data providers (DP1, DP2 ...) will identify cookie_ids as belonging to a specific demographic segment, say DP1_S1 = F35-44. To test the accuracy of this assignment, random samples of cookie_ids in these segments are measured against an industry standard source of "ground truth". This will yield data like "40% of the cookie_ids that DP1 identifies as in S1 are actually (according to "ground truth") in F35-44" and "30% of the cookie_ids that DP2 identifies as in S5 are actually (according to "ground truth") in F35-44". Given the above, what is the probability that a cookie_id that is in both DP1_S1 and DP2_S5 will be found to be in F35-44 according to the industry standard source of ground truth?

