# models

What type of problem does the model try to solve?



Is it prone to over-fitting? If so – what can be done about this?



Does the model make any important assumptions about the data? When might these be unrealistic? How do we examine the data to test whether these assumptions are satisfied?



Does the model have convergence problems? Does it have a random component or will the same training data always generate the same model? How do we deal with random effects in training?



What types of data (numerical, categorical etc…) can the model handle?



Can the model handle missing data? What could we do if we find missing fields in our data?



How interpretable is the model?



What alternative models might we use for the same type of problem that this one attempts to solve, and how does it compare to those?



Can we update the model without retraining it from the beginning?



How fast is prediction compared to other models? How fast is training compared to other models?



Does the model have any meta-parameters and thus require tuning? How do we do this?



What is linear in a generalized linear model?



What is a probabilistic graphical model? What is the difference between Markov networks and Bayesian networks?



Give an example of an application of non-negative matrix factorization



On what type of ensemble technique is a random forest based? What particular limitation does it try to address?



What methods for dimensionality reduction do you know and how do they compare with each other?



What are some good ways for performing feature selection that do not involve exhaustive search?



How would you evaluate the quality of the clusters that are generated by a run of K-means?



Why do we need/want the bias term?



Why do we call it GLM when it's clearly non-linear? (somewhat tricky question, to be asked somewhat humorously---but extremely revealing.)