## Data exploration

How do you summarize the distribution of your data?



How do you handle outliers or data points that skew data?



What assumptions can you make? Why and when? (i.e When is it safe to assume "normal")



## Biases

When you sample, what bias are you inflicting?



How do you control for biases?



What are some of the first things that come to mind when I do X in terms of biasing your data?



## Process

How do you select features?



How do you evaluate a model?



Train/test/validate sets



Cross-validation



## Regularization

What is Regularization?



Which problem does Regularization tries to solve?



What is the difference in the outcome (coefficients) between the L1 and L2 norms? (usually I ask them to draw the geometric shape of the functions, just to make sure)



## Parametric learning

What is the Gradient Descent Method (the intuition is mostly enough)?



Does the Gradient Descent method always converge to the same point?



Is it necessary that the Gradient Descent Method will always find the global minima?



How maximum likelihood works?



Whats EM?



## Scalability

1. What is the curse of higher dimensionality?



2. What is the difference between density-sparse data and dimensionally-sparse data?



3. What does "higher dimensionality" imply when applying textbook clustering algorithms developed for low dimension metric spaces to, say, numerical text analysis? Think of using cluster density to identify "good" clusters.


