{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.datasets import fetch_20newsgroups, fetch_rcv1\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_IDF      = True\n",
    "USE_HASH     = True\n",
    "MAX_FEATURES = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "True k:  20\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(len(dataset.data))\n",
    "print(\"True k: \", true_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "en_stop = dict()\n",
    "for i in nltk.corpus.stopwords.words('english'): en_stop[i] = True\n",
    "    \n",
    "def tokenize(text):\n",
    "    text      = \" \".join(text.split(\"\\n\\n\")[1:]).lower()\n",
    "    tokenizer = nltk.word_tokenize\n",
    "    stemmer   = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    words     = [ stemmer.stem(token)\n",
    "        for token in tokenizer(text)\n",
    "        if not en_stop.has_key(token) and re.match('^[a-z]+$', token) \n",
    "    ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec   = CountVectorizer(stop_words='english', max_features=MAX_FEATURES)\n",
    "X     = count.fit_transform(dataset.data)\n",
    "vocab = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hasher = HashingVectorizer(stop_words='english', non_negative=True, norm=None, binary=False, n_features=MAX_FEATURES)\n",
    "vec    = make_pipeline(hasher, TfidfTransformer())\n",
    "X      = vec.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec   = TfidfVectorizer(tokenizer=tokenize,  max_df=0.5, min_df=.2, stop_words='english',\n",
    "                        use_idf=True, max_features=MAX_FEATURES)\n",
    "X     = vec.fit_transform(dataset.data)\n",
    "vocab = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 18846 documents\n",
      "Starting trainning\n",
      "CPU times: user 7min 9s, sys: 2.45 s, total: 7min 12s\n",
      "Wall time: 7min 16s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.033*\"key\" + 0.022*\"use\" + 0.016*\"encrypt\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.040*\"israel\" + 0.034*\"jew\" + 0.029*\"isra\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.021*\"drive\" + 0.016*\"use\" + 0.015*\"card\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.032*\"x\" + 0.022*\"imag\" + 0.022*\"file\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.013*\"use\" + 0.011*\"write\" + 0.009*\"articl\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.029*\"window\" + 0.021*\"use\" + 0.013*\"run\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.021*\"space\" + 0.011*\"orbit\" + 0.009*\"launch\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.016*\"gun\" + 0.011*\"peopl\" + 0.010*\"articl\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.016*\"fire\" + 0.012*\"would\" + 0.012*\"write\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.016*\"car\" + 0.011*\"write\" + 0.011*\"get\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.015*\"univers\" + 0.012*\"inform\" + 0.009*\"rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.029*\"game\" + 0.019*\"team\" + 0.014*\"year\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.024*\"would\" + 0.017*\"know\" + 0.016*\"write\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.014*\"san\" + 0.014*\"la\" + 0.011*\"gm\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.024*\"god\" + 0.012*\"christian\" + 0.010*\"one\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.014*\"armenian\" + 0.010*\"peopl\" + 0.009*\"war\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.016*\"use\" + 0.014*\"medic\" + 0.012*\"diseas\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.012*\"govern\" + 0.010*\"state\" + 0.009*\"presid\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.023*\"write\" + 0.020*\"moral\" + 0.019*\"articl\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.060*\"max\" + 0.058*\"r\" + 0.051*\"g\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                                                  1\n",
       "0    0        0.033*\"key\" + 0.022*\"use\" + 0.016*\"encrypt\"\n",
       "1    1        0.040*\"israel\" + 0.034*\"jew\" + 0.029*\"isra\"\n",
       "2    2         0.021*\"drive\" + 0.016*\"use\" + 0.015*\"card\"\n",
       "3    3            0.032*\"x\" + 0.022*\"imag\" + 0.022*\"file\"\n",
       "4    4       0.013*\"use\" + 0.011*\"write\" + 0.009*\"articl\"\n",
       "5    5         0.029*\"window\" + 0.021*\"use\" + 0.013*\"run\"\n",
       "6    6     0.021*\"space\" + 0.011*\"orbit\" + 0.009*\"launch\"\n",
       "7    7       0.016*\"gun\" + 0.011*\"peopl\" + 0.010*\"articl\"\n",
       "8    8       0.016*\"fire\" + 0.012*\"would\" + 0.012*\"write\"\n",
       "9    9          0.016*\"car\" + 0.011*\"write\" + 0.011*\"get\"\n",
       "10  10  0.015*\"univers\" + 0.012*\"inform\" + 0.009*\"rese...\n",
       "11  11         0.029*\"game\" + 0.019*\"team\" + 0.014*\"year\"\n",
       "12  12       0.024*\"would\" + 0.017*\"know\" + 0.016*\"write\"\n",
       "13  13              0.014*\"san\" + 0.014*\"la\" + 0.011*\"gm\"\n",
       "14  14      0.024*\"god\" + 0.012*\"christian\" + 0.010*\"one\"\n",
       "15  15     0.014*\"armenian\" + 0.010*\"peopl\" + 0.009*\"war\"\n",
       "16  16       0.016*\"use\" + 0.014*\"medic\" + 0.012*\"diseas\"\n",
       "17  17    0.012*\"govern\" + 0.010*\"state\" + 0.009*\"presid\"\n",
       "18  18     0.023*\"write\" + 0.020*\"moral\" + 0.019*\"articl\"\n",
       "19  19                0.060*\"max\" + 0.058*\"r\" + 0.051*\"g\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import *\n",
    "from gensim.models.ldamulticore import *\n",
    "from gensim.models.ldamodel import *\n",
    "\n",
    "texts = [ tokenize(text) for text in dataset.data ]\n",
    "print(\"Parsed %d documents\" % len(texts))\n",
    "\n",
    "dictionary = Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.6)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "print(\"Starting trainning\")\n",
    "\n",
    "%time ldamodel = LdaModel(corpus, num_topics=true_k, id2word=dictionary, passes=20)\n",
    "pd.DataFrame(ldamodel.print_topics(num_topics=true_k, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "* how to assess performance\n",
    "* optimize parameters(k, filters, ...)\n",
    "* visualize results\n",
    "* predict new instances\n",
    "* explore library\n",
    "* train bigdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://de.dariah.eu/tatom/topic_model_python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X_c\n",
    "\n",
    "for i in [true_k-2, true_k-1,true_k, true_k+1, true_k+2]:\n",
    "    nm = NMF(n_components=num_topics, random_state=1)\n",
    "    docs = nm.fit_transform(dtm)\n",
    "    \n",
    "#     topic_words = []\n",
    "\n",
    "#     for topic in clf.components_:\n",
    "#         word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "#         topic_words.append([vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======  18\n",
      "Homogeneity: 0.288\n",
      "Completeness: 0.388\n",
      "V-measure: 0.331\n",
      "Adjusted Rand-Index: 0.092\n",
      "Silhouette Coefficient: 0.006\n",
      "=======  19\n",
      "Homogeneity: 0.312\n",
      "Completeness: 0.375\n",
      "V-measure: 0.340\n",
      "Adjusted Rand-Index: 0.119\n",
      "Silhouette Coefficient: 0.006\n",
      "=======  20\n",
      "Homogeneity: 0.260\n",
      "Completeness: 0.315\n",
      "V-measure: 0.285\n",
      "Adjusted Rand-Index: 0.099\n",
      "Silhouette Coefficient: 0.006\n",
      "=======  21\n",
      "Homogeneity: 0.295\n",
      "Completeness: 0.345\n",
      "V-measure: 0.318\n",
      "Adjusted Rand-Index: 0.113\n",
      "Silhouette Coefficient: 0.007\n",
      "=======  22\n",
      "Homogeneity: 0.277\n",
      "Completeness: 0.337\n",
      "V-measure: 0.304\n",
      "Adjusted Rand-Index: 0.094\n",
      "Silhouette Coefficient: 0.007\n"
     ]
    }
   ],
   "source": [
    "X = X_hi\n",
    "\n",
    "for i in [true_k-2, true_k-1,true_k, true_k+1, true_k+2]:\n",
    "    km = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1, n_jobs=-1)\n",
    "    km.fit(X)\n",
    "    print(\"======= \", i)\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "    print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "    print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "    print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, km.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if USE_LSA:\n",
    "#     original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "#     order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vec.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/mblondel/1451300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
